\section{Research Design}

In this section, we present our research plan. We shall begin by developing a model of statistical decision-making that is adapted to the particular problems of evaluating forensic procedures, where the usual statistical framework of Type I or Type II error does not fit very comfortably. Our measures of forensic performance arise directly from this model.  Second, we shall describe our test set of election data. Finally, we shall explain the procedures that we evaluated. Resource constraints only allowed us the opportunity to focus on the techniques that are most commonly used and most straightforward to apply, which is to say those digit tests that are based on the Benford laws, and our own innovations.  The greatest focus in this last section will be on explaining the intuition behind our Bayesian approach.

\subsection{A Decision-Theoretic Framework for Evaluating Forensic Procedures}

One of the most significant problems for the literature is uncertainty about where fraud is happening and the implications of that uncertainty for the quality of the procedures that are used. There are countries where it is thought that fraud is happening a lot and where we think it is happening rarely, but in any particular place we are ultimately unsure about whether fraud has really happened there. Such uncertainty makes the process of evaluating a forensic procedure subjective and non-transparent. What we are doing in evaluating how procedures perform on real election data is not exactly assessing Type I (Type II) error, because of the possibility that fraud might be present (not present) in our election data. We think that developing a decision theoretic framework leads to more coherent and transparent understanding of what we are trying to accomplish with evaluation, and leads to better founded measures of the performacne of electoral fraud detection procedures.  

Suppose an applied researcher has collected election returns from $p$ geographic areas in country $c$, which we shall denote by $\{\mathcal{D}_1,\mathcal{D}_2,\ldots,\mathcal{D}_p \}$.  The investigator plans to apply a forensic procedure $\delta$ to each election return, and we will assume that he believes $\delta(\mathcal{D}_i) \sim \textrm{Bernoulli}(\tilde{\theta}_c)$, in other words that whether the procedure finds fraud is a coin-flip whose odds of success depends on some country-specific probability $\tilde{\theta}_c$, which he interprets as the probability of fraud in that country. This interpretation is naive, because in truth the probability that a forensic procedure detects fraud depends not only on the proportion of fraudulent elections there are in the country, but also on the power and accuracy of the procedure used, however we think it is also fairly obvious that many applied researchers will interpret election forensic tools in this fashion. We suppose the investigator has a prior belief that $\tilde{\theta}_c \sim \textrm{Beta}(\tilde{\alpha},\tilde{\beta})$.  If the investigator is rationally updating his beliefs, then after observing $\{\delta(\mathcal{D}_1),\delta(\mathcal{D}_2),\ldots,\delta(\mathcal{D}_p) \}$ he or she believes that $$\tilde{\theta}_c \sim \textrm{Beta} \left(\tilde{\alpha}+\sum_{i=1}^p \delta(\mathcal{D}_i),\tilde{\beta}+p-\sum_{i=1}^p \delta(\mathcal{D}_i) \right)$$

Now a political methodologist is considering recommending the investigator use one of several possible procedures $\delta$. She wants to know the costs incurred by various choices. The observing methodologist does not know either the true proportion of fraudulent cases $\theta_c$, nor does she have any plausible model of how $\delta(\mathcal{D}_i)$ relates to $\theta_c$. Any such model would involve assumptions about the power of election forensic procedures, which to our knowledge have not been proposed and seem to us impossible to evaluate. For this reason, the only thing a political methodologist can do is to consider as a cost the difference between what the applied researcher will believe and various possible true values for the parameter $\theta_c$.\footnote{Formulating the decision problem as we have may result in cognitive dissonance for some as the applied researcher has beliefs about a paremeter while the methodologist does a very un-Bayesian thing of assuming that the parameter $\theta_c$ is a constant. The primary justification for this is that the formulation we are presenting is the most efficient and tractable. By making the applied researcher a Bayesian, we get the flexibility to define measures comparing the strength of posterior belief.  By allowing the methodologist to reason about ``true'' parameter values, we avoid specifying statistical relationships that would require other, less plausible modeling assumptions.}

With the problem so formulated, one can put on surer epistemological footing various ways of evaluating $\delta$. Suppose we take an absolute value metric, for example, between a hypothetical $\theta_c$ true value and $f(\tilde{\theta}_c)$, some function of the applied researchers belief.  This suggests the cost

\begin{equation}
C (\theta_c,\delta,f,\tilde{\alpha},\tilde{\beta}) = \int_\Theta \left| \theta_c - f(\tilde{\theta}_c) \right| d\tilde{\theta}_c \label{eq:cost}
\end{equation}

For example, it is commonly supposed that if $c$ is an advanced industrial democracy with well-developed political institutions and no systematic reports of fraud, then $\theta_c \approx 0$. In cases where $\theta_c=0$, and $f$ is the identity, the ``cost'' of a procedure is the applied researcher's expectation about the proportion of fraud in that country.  Assuming the applied researcher has no prior beliefs at all, so that $\tilde{\alpha} = \tilde{\beta} = 0$, the cost reduces further to the proportion of elections deemed fraudulent:

$$C(0,I,\delta,0,0) =\frac{\sum_{i=1}^p \delta(D_i)}{p}$$

One of the primary ways that methodologists seem to have been justfying their procedures is by using something like $C(0,I,\delta,0,0)$. \textcite{Shikano2011}, for example, argue that a forensic procedure that uses an inflated $\chi^2$ statistic is preferrable to one that does not because the unadjusted $\chi^2$ finds too many fraud. \textcite{Beber2012} argues that last digit tests are valid because the cost $C(0,f,\delta,0,0)$ in Sweden is not too different from $0.05$, what one would expect from a 95\%-level test. 

Besides increasing the transparency of how methodologists have chosen between procedures without relying on potentially inappropriate references to Type I error, the real strength of this general formulation is that it allows us to develop other ways of comparing procedures, again in a way that is transparent about what we are doing. For example, if $\delta_1$ detects fraud less frequently than $\delta_2$ in a country without fraud, we can compare how much stronger an applied researcher's prior belief would have to be in using the second procedure to get the same cost outcome. The hard part of doing this is figuring out how to quantify strength of prior belief.  Fortunately, the Beta distribution has a parameterization that allows us to do this in a way that is quite natural. Let $\tilde{\mu} = \frac{\tilde{\alpha}}{\tilde{\alpha}+\tilde{\beta}}$ be the prior mean and $\tilde{\nu} = \alpha+\beta$ be the prior sample size. $\tilde{\nu}$ gets its name as the prior sample size because of the properties of the posterior of the Beta-Binomial distribution. If $n$ Bernoulli trials are observed, then in the posterior the value of $\nu$ will be $\alpha+\beta+n$, so for fixed $\mu$ the prior sample size rougly says how many observed outcomes that prior is based on.  If we fix the expectation of the prior belief at $\mu$, we can calculate:

\begin{gather}
\nu_{\mu,k,\delta_1}^*(\delta_2,\theta_c) = \arg \min_{\tilde{\nu} \ge 0} C(\theta_c,\delta_2,I,\mu \tilde{\nu},(1-\mu)\tilde{\nu})  \\
\textrm{subject to } \notag \\
C \left(\theta_c,\delta_2,I,\mu \tilde{\nu},(1-\mu)\tilde{\nu} \right) \le  C \left(\theta_c,\delta_1,I,\mu k,(1-\mu)k \right) \notag
\end{gather} 

For our set of low-fraud countries, we will report $\nu_{0.01,1,\delta^*} (\delta,0)$ which measures how many additional prior information an applied researcher would have to bring to the table in using $\delta$ in order to get a lower cost than an applied researcher who had a weakly informative prior and used $\delta^*$, assuming both researchers expected about 1\% of elections to be fraudulent. Obviously, there are many degrees of freedom in such models, but graphical procedures can help us illustrate that our findings are really about the relative strength of the forensic procedures and not about the way we are measuring costs. 

For most countries, it is not reasonable to think that fraud never happens. Only considering the cost of $\theta_c=0$ rewards only those procedures that are weak; indeed ,it suggests that the best procedure is one that says fraud never occurs. To eliminate the preferrability of such procedures, a methodologist can average Equation $\ref{eq:cost}$ over a range of possible values for $\theta_c$, possibly even a weighted to reflect previous knowledge such as reports from election observors. We do not do this, because we lack such knowledge, however we do think that if others were to do this it would increase the transparency of arguments about the magnitude of the costs of failing to detect fraud. 

Clearly, however, some way of assessing the ability of a test to detect fraud when it is genuinely happening is important. Our primary way of evaluating procedures on the basis of being too weak is through their performance on data that we have doctored ourselves, in other words a dataset for which we know $\theta_c=1$.

We think that balancing these metrics and others like them can give a good assessment of the performance of election forensic methods.  
\subsection{Data Sources}



